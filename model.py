# -*- coding: utf-8 -*-
"""kernel-ocr-hindichanged.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lxgcR8k9--8oI_u2SjqVhBO4D5TooPWY
"""

!pip install kaggle
 
#upload the kaggle json file
!pip install kaggle
!rm -rf ~/.kaggle
!pwd
import json
token = {"username":"rochak123","key":""} #create in kaggle setting
with open("kaggle.json", "w") as file:
  json.dump(token, file)
 
!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.json
 
 
 
 
#link of datset
##it will be added in zip form
!kaggle datasets download -d jhashanku007/devnagri-hindi-dataset
# !unzip devnagri-hindi-dataset.zip

!unzip devnagri-hindi-dataset.zip

from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import LearningRateScheduler, ModelCheckpoint

path = 'DevanagariHandwrittenCharacterDataset'

os.listdir('DevanagariHandwrittenCharacterDataset/')

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   rotation_range = 10,
                                   zoom_range = 0.15,
                                   width_shift_range =0.1,
                                   height_shift_range =0.1,
                                   horizontal_flip = False,
                                   validation_split = 0.13)

test_datagen = ImageDataGenerator(rescale = 1./255)

train_set = train_datagen.flow_from_directory(path + '/Train', target_size = (64, 64), batch_size = 32, class_mode = 'categorical', subset='training',color_mode = 'grayscale')

validation_set = train_datagen.flow_from_directory(path + '/Train', target_size = (64, 64), batch_size = 32, class_mode = 'categorical', subset='validation',color_mode = 'grayscale')

test_set = test_datagen.flow_from_directory(path + '/Test', target_size = (64, 64), batch_size = 32, class_mode = 'categorical' , color_mode = 'grayscale'  )

Labels = train_set.class_indices
print(Labels)
num_classes = len(Labels)

Characters = 'क ख ग घ ङ च छ ज झ ञ ट ठ ड ढ ण त थ द ध न प फ ब भ म य र ल व श ष स ह क्ष त्र ज्ञ ० १ २ ३ ४ ५ ६ ७ ८ ९'

Characters = Characters.split(' ')

key_list = list(Labels.keys())
key_list

final = []
for i in Labels:
  value = i.split('_')[1]
  final.append(Characters[int(value)])
  print(value)#Labels[int(value)])
  
print(final)

import re
def atoi(text):
    return int(text) if text.isdigit() else text
def natural_keys(text):
    return [ atoi(c) for c in re.split('(\d+)',text) ]
ls = os.listdir(path+'/Train')
ls.sort(key = natural_keys)

print(ls,end  ='')

Cha_Uni = {}
for j,i in enumerate(ls):
    Cha_Uni[i] = Characters[j]

Cha_Uni

yy =20
a = key_list[int(yy)]
print(a,Cha_Uni[a])

model = Sequential()

model.add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = (64,64,1)))
model.add(BatchNormalization())
model.add(Conv2D(32, kernel_size = 3, activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.4))

model.add(Conv2D(64, kernel_size = 3, activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(64, kernel_size = 3, activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.4))

model.add(Conv2D(128, kernel_size = 4, activation='relu'))
model.add(BatchNormalization())
model.add(Flatten())
model.add(Dropout(0.4))
model.add(Dense(num_classes, activation='softmax'))

# COMPILE WITH ADAM OPTIMIZER AND CROSS ENTROPY COST

checkpoint_filepath = 'contents/'
model_checkpoint_callback = ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)
model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
history = model.fit(train_set, epochs=40, verbose=1, validation_data = validation_set, shuffle=True,callbacks=[model_checkpoint_callback])

# model = Sequential()
# model.add(Conv2D(64, (3, 3), padding='same', input_shape=(64, 64, 1), activation = 'relu'))
# model.add(Conv2D(64, (3, 3), activation = 'relu'))

# model.add(MaxPooling2D(pool_size=(2, 2)))
# model.add(Dropout(0.25))

# model.add(Flatten())
# model.add(Dense(128, activation = 'relu'))
# model.add(Dropout(0.2))
# model.add(Dense(64, activation = 'relu'))
# model.add(Dense(num_classes, activation = 'softmax'))

# model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
# history = model.fit(train_set, epochs=80, verbose=1, validation_data = validation_set, shuffle=True)

import matplotlib.pyplot as plt


# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

model_json = model.to_json()
with open("model.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("model(weights).h5")
print("Saved model to disk")

#full model save
model.save("model.h5")

'''
from keras.models import model_from_json
# load json and create model
json_file = open('/kaggle/working/model.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
# load weights into new model
loaded_model.load_weights("/kaggle/working/model.h5")
print("Loaded model from disk")
'''

test_loss , test_accuracy = model.evaluate(test_set)

print("Test accuracy: "+ str(test_accuracy*100), "\nTest_loss: " + str(test_loss*100))



from skimage.transform import resize
from PIL import Image
import numpy as np
im = Image.open(path+'/Test/character_28_la/11454.png')
im = np.array(im)
im = resize(im , (1,64,64,1))

y_pred = model.predict(im)

yy = np.argmax(y_pred)
print(yy)

print(key_list[int(yy)])